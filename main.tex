\documentclass{article}
\usepackage{nips_2016}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography



\newcommand{\tell}{\ensuremath{\tilde{\ell}}}

\newcommand{\w}{\mathbf{w}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}{{\!\top}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\Exy}{\ensuremath{\mathbb{E}_{\x,y}}}
\newcommand{\BigBracks}[1]{\Bigl[#1\Bigr]}



\title{Learning to gentoype}

\author{
Nicol\'as Della Penna
\And
Erik Garrison \\
} %equal contributors


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
a novel feature representation
for a space which is hugely important in terms of production
but has thus far not really had anything similar to work from
freebayes and fermkit one liner
enriched with features from gui inspired representation
FDA state of the art
\end{abstract}

\section{Introduction}


%motivation

% TODO:why you care about genotyping,1 line.
Genome sequencing has rapidly become a core observational process in biology and medicine.
Sequencing and assembling a genome \emph{de novo} is dramatically more expensive than \emph{re}sequencing, in which the fragments of a newly-sequenced genome are aligned against an existing assembled reference genome).
As the error rates of sequencing methods are typically much higher than the rate of actual genetic variation, it is necessary to classify the candidate variations which are implied by the alignments on the basis of their veracity.
Organisms (such as humans) frequently have two or more nearly-identical copies of each genetic locus, and as a result we reason about their genome in as a series of genotypes, which are multisets of alleles $G = \{ a_1, \ldots, a_N \}$.\footnote{The same allele can occur more than once in diploids and polyploids.} 
% TODO: what learning task is genotyping ;   classification number of possible allelic states,

Let $N$ be the number of genome copies at a genomic locus, $A$ be the set of alleleic states where an alleles is a sequence of DNA, and $E$ be the set of degenerate or unknown states.
We obtain sequencing information $S$ relevant to our task by aligning small fragments of DNA to a reference genome \cite{li2013bwamem} or by building an assembly graph that compresses the input sequencing data before contextualizing it relative to the reference \cite{myers2005, simpson2010, li2015fermikit, iqbal2012}.
\emph{Genotyping} can be seen as an $N \times ( |A| + |E| )$-way classification task in which we use a model $f$ to obtain a mapping $f(S) \to G$.

%We can genotype a sample 

Standard approaches to developing $f$ use Bayesian filters derived from idealized \emph{a-priori} models of the genome  \cite{samtools, gatk2011, garrison2012haplotype, rimmer2014integrating, li2011stats}.
%This approach is sensible where we have little prior information to inform $f$, but ..
It is no longer the case that the best prior available for genome resequencing is one that we derive from first principles. 
Large scale genomic surveys have sequenced hundreds of thousands of humans, and where possible have deposited results into the public domain \cite{1000Gphase1, 1000g2015, exac2015, cavalli2005human, uk10k2015uk10k}.
The NIST Genome in a Bottle consortium \cite{zook2014integrating} has developed a reference standard for genome resequencing based around NA12878 (a cell line derived from a HapMap participant \cite{gibbs2003international}), which provides an accurate set of labeled data for the evaluation of genomic analyses.
Here, we develop a model capable of using these resources to learn a general model for genotyping.
%To utilize this rapidly-deepening catalog of variation in a new individuals, we built a supervised learning task to genotype by example.
% this has been attempted before, as in GATK VQSR or SNPsvm TODO: cites? do we beat VQSR?
%To attack this task we develop a novel set of feature representations, use the complementary strengths of two variant callers,  achieving state of the art performance.

% The system trained in this way, using the complementary strengths of two variant callers, freebayes and fermkit, combined with a novel set of feature representations inspired by genomic debugging tools and graphical reductions of genome variation

%President Obamaâ€™s Precision Medicine Initiative envisions a day when an individual's medical care will be tailored in part based on their unique characteristics and genetic make-up.
%The goal of the FDA's first precisionFDA challenge is to engage the genomics community in advancing the quality standards in order to achieve more consistent results in the context of genetic tests (related to whole human genome sequencing), advancing the goal of better personalized care.
%PrecisionFDA invites all innovators to take the challenge and assess their software on the supplied reference human datasets. Participation is voluntary, but instrumental in helping the community prepare for the coming genomic data revolution.

The PrecisionFDA challenge is a publicly-funded initiative to improve the quality and consistency of genetic tests through the engagement of the genomics community in a series of open challanges.
Its objective is to bring the same kind of rigorous public testing that has been used by the machine learning community to the medical genomics community.
We have applied our approach to the initial phase of the challenge (the ``Consistency'' challenge) and achieve the highest precision of any method which explicitly did not use the truth set in any phase of the submission process.\footnote{There was an entry that achieves 100 \% precision and recall by simply repeating the evlaution set, which was sadly distributed at the beginning of the challenge. Other submissions provide no more than vague reference to a ``ML'' approach used to filter, and do not exclude the use of the truth set for final decision boundary selection.}

Our models base approach achieving the highest precision in the initial challenge (The precision challenge) of an entry that explciitly does not use the evaluation set in its training or paramter tunning 
%TODO https://precision.fda.gov/comparisons/1030 is problematic since they claim not to be using the test set (for training, i cosen my words crefully nd i assume so did they) 


We require a corpus of examples of sequencing data from genomic loci where we have a strong estimate of the actual genome state. %TODO: edit flow with upper part
% this is available in NIST genome in a bottle
% background of problem
% history: 1000G, data formats, NIST
% current approaches 
% reference guided: bwa / aligners -> samtools, freebayes, platypus, GATK % assembly based: cortex, sga, fermikit
 similar techniques: GATK's VQSR (variant quality score recalibration) and SNPSVM (mention), but not a challenge particpant)
% VSQR provide rough idea of what is different http://gatkforums.broadinstitute.org/gatk/discussion/39/variant-quality-score-recalibration-vqsr
The performance is comparable to that achieved by GATK with the recomended methodology; it offers several adavantges; libre, much smaller human effort, tunable to new sequencing tenchnologies (TODO: so is GATK's VQSR ...) 


%Application papers should describe your work on a "real" as opposed to "hypothetical" application; specifically, it should describe work that has direct relevance to, and addresses the full complexity of, solving a non-trivial problem. Authors are also encouraged to convey insight about the problem, algorithms, and/or application

% elucidate (through an ablative analysis/lesion analysis, which removes one component of an algorithm at a time) which were the key components of the system needed to get the application to work. 
The features provided by the two underlying aligners, combined with features that characterize the alignmnt itself, can be used to make a a good genotyper.



%A NIPS application paper should be comparable in quality to paper in the corresponding application domain conference: for example, a text paper should be acceptable to SIGIR, EMNLP, or other appropriate conference Application papers should not only present concrete application results, but also contain at least one of the below elements:

%Applications that couldn't previously be done, at all, or on this scale
TODO can we argue somethign of what we are doing could be done before? at least not with libre software? or possibly theres some way to argue this.


%Techniques shown to be uniquely fitted to specific popular applications, leading to improved performance or more accurate solutions
%our strong point

%Insights that, from the perspective of machine learning, distinct applications X and Y, whose respective users have never talked to each other, are the same.
%cant see anythign like this, but leaving it for inspiration




%dna/gene literature in NIPS



\section{Pipeline from sequencing machine to dataset}

% notes from precisionFDA

%We use hhga to transform these examples into a representation suitable for linear learning,  stochastic gradient descent as implemented in Vowpal Wabbit (VW), to learn a mapping between sequencing data and genotype.

For the FDA precision challenge, we trained the models using three previous Garvan sequencing runs on NA12878.
First the DATA (WHICH?) is filtered to retain only likely-variable regions in the input. TODO: how exactly is this decided?
The union set of candidate variant calls using alignment (bwa mem and freebayes) and assembly-based (fermikit) processes is then created.
These candidate loci are labeled with their genotype status in the NIST Genome in a Bottle v2.19 truth set. 

TODO: add description of how the different featurespaces we keep are constructed.

\section{Learning}

\begin{equation}
  \min_{\w \in \R^d}~~\sum_{i=1}^n\ell(\w^\tr\x_i;\,y_i) 
  \label{eqn:objective}
\end{equation}

The learning from the data is implemented without explicit regularization penalty, which is instead achieved by stopping the SGD when the holdout error rate is not reduced over three iterations \citep{hardt2015train}.
The main reasoning behind this choice was to avoid hyper-parameter tuning given the computational cost of fitting the richer models.
%TODO CPU hours for the stage poly?
Error-correcting tournaments where used
\citep{beygelzimer2009error}, showed better performance in the holdout than the standard one against all reductions. 
Feature hashing and stochastic gradient descent using adaptive normalized and invariant update scheme as implemented in Vowpal Wabbit \cite{mcmahan2010adaptive, duchi2011adaptive, agarwal2014reliable} 



\section{Results}

Highest precision submission among those that explicitly dont use the test set in their learning or tuning.\footnote{ Without this restriction there is a completely accurate fiel submitte,d as well as several methods whose peroformance is unbelievably good TODO: show more explcitily why t is implausible, and where the degree to which the adjustments are tuned to roduce good results in the evaluation set is left unsaid.}

\subsection{Baseline}
 
 --ignore sg --ect 7 -d robots134n.shuf.hhga.gz -f allbutsgrobots.model --passes 100 -c
 average loss = 0.016359 h                       
vial1 0.016243

average loss = 0.007829
total feature number = 2163159720
vw -t -i ffoz.model -d vial1l.hhga.gz  18075.76s user 16.10s system 104% cpu 4:47:43.55 total

%TODO make robots134o.30 holdout regions ?
% name=qqonly; input=robots134o.30; mkdir -p $name; date >$name.started; paste <(seq 0 29) <(find $input -type f | grep gz$ ) | parallel -j 30 'dir='$name'; id=$(echo {} | cut -f 1); chunk=$(basename $(echo {} | cut -f 2)); echo $id, $chunk ; vw --total 30 --node $id --unique_id 99763211 -d '$input'/$chunk --cache_file $dir/$chunk.cache  --ect 7  --passes 100 --compressed  --keep m -q qq  --span_server localhost $(if [ $id -eq 0 ]; then echo -f '$name'.model --save_per_pass ; fi) 2>'$name'.$id.log' ; vw -t -i '$name'.model -d vial1 


\subsection{Ablation}
% all run for --passes 20 --ect 7 -b 23 and the holdout on
% the train set with the regions removed
%

\begin{table}[]
  \centering
  \caption{Ablation results}
  \label{ablation-results}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    name & binary loss & features & interactions & options \\ \hline
    3f1c & 0.012204 & s & ss & \\
    6678 & 0.407825 & r & rr &\\
    37e4 & 0.016569 & a & & \\
    7d9d & 0.416780 & h & hh & \\
    a581 & 0.035268 & k & kk & \\
    85e8 & 0.024088 & m & mm & \\
    140f & 0.009975 & ms & mm  ss  ms & \\
    7a6a & 0.021169 & km & kk  mm  km & \\
    7a24 & 0.009639 & kms & kk  km  mm  ms  ss & \\
    0031 & 0.008795 & kmsa & kk  km  mm  ms  ss & \\
    bd23 & 0.009060 & kmshr & kk  km  mm  ms  ss  hr & \\
    10d3 & 0.009205 & kmshr & kk  km  mm  ms  ss  hr & ngram 2 \\
    b9fd & 0.009368 & kmshr & kk  km  mm  ms  ss  hr & ngram 3 \\
    431f & 0.008799 & kmshra & kk  km  mm  ms  ss  hr & ngram 2 \\
    4b31 & 0.012261 & kmshr & & stage-poly \\
    \hline
  \end{tabular}
\end{table}

% can't do ... maybe we report it -- effect of using "clever" featuranmes for aln relative to just some natural ordering.


\section{Generalization}

\subsection{Across sequencing runs}



\subsection{Accross individuals}
where there are actually reads we can use to train and test on

%zcat 1kg/HG00403.hhga.gz | head -1000000 | grep 'aln0.4\|aln1.4\|aln2.4' | vw --ect 7 --ignore sg --cache_file x.cache -k --passes 10 -q kk -q mm -f x.model

makes sure we have 4 alignments in at least one allele bin

\subsection{Accross regions}
there is a better way
we can hold out specific regions
but the individual
at this level
is DNA that was synthesized in a test tube

it is not like there is something special about it
we can show that we don't need to memorize sites
TODO:SHOW

that's enough to make a strong claim that you would generalize to a new sample with different variants in different places
if you want to go so far
obviously it has caveats but if you want to explore this that's the cleanest way

that DNA is a molecule and when a person has a variatn in a new position it is indistinguishable from another variant we held out from the current individual

the itner-individual comparison in the 1000G is fraught too
because the samples were all genotyped in a massive joint assembly process
so it's hard to claim they are independent

\section{Conclusion}

using simple learners and leveraging of tools currently applied to the task to automate remaning parts


\section{Further Work}
incoporating 1000G

\section*{References}
\small

\bibliographystyle{plain}
\bibliography{geno}

\end{document}


