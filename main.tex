\documentclass{article}
\usepackage{nips_2016}
% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography



\newcommand{\tell}{\ensuremath{\tilde{\ell}}}

\newcommand{\w}{\mathbf{w}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}{{\!\top}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\Exy}{\ensuremath{\mathbb{E}_{\x,y}}}
\newcommand{\BigBracks}[1]{\Bigl[#1\Bigr]}



\title{Learning to gentoype}

\author{
Nicol\'as Della Penna
\And
Erik Garrison \\
} %equal contributors


\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
a novel feature representation
for a space which is hugely important in terms of production
but has thus far not really had anything similar to work from
freebayes and fermkit one liner
enriched with features from gui inspired representation
FDA state of the art
\end{abstract}

\section{Introduction}


%motivation

% TODO:why you care about genotyping,1 line.
Genome sequencing has rapidly become a core observational process in biology and medicine.
Sequencing and assembling a genome \emph{de novo} is dramatically more expensive than \emph{re}sequencing, in which the fragments of a newly-sequenced genome are aligned against an existing assembled reference genome).
As the error rates of sequencing methods are typically much higher than the rate of actual genetic variation, it is necessary to classify the candidate variations which are implied by the alignments on the basis of their veracity.
Organisms (such as humans) frequently have two or more nearly-identical copies of each genetic locus, and as a result we reason about their genome in as a series of genotypes, which are multisets of alleles $G = \{ a_1, \ldots, a_N \}$.\footnote{The same allele can occur more than once in diploids and polyploids.} 
% TODO: what learning task is genotyping ;   classification number of possible allelic states,

Let $N$ be the number of genome copies at a genomic locus, $A$ be the set of alleleic states where an alleles is a sequence of DNA, and $E$ be the set of degenerate or unknown states.
We obtain sequencing information $S$ relevant to our task by aligning small fragments of DNA to a reference genome \cite{li2013bwamem} or by building an assembly graph that compresses the input sequencing data before contextualizing it relative to the reference \cite{myers2005, simpson2010, li2015fermikit, iqbal2012}.
\emph{Genotyping} can be seen as an $N \times ( |A| + |E| )$-way classification task in which we use a model $f$ to obtain a mapping $f(S) \to G$.

%We can genotype a sample 

Standard approaches to developing $f$ use Bayesian filters derived from idealized \emph{a-priori} models of the genome  \cite{samtools, gatk2011, garrison2012haplotype, rimmer2014integrating, li2011stats}.
%This approach is sensible where we have little prior information to inform $f$, but ..
It is no longer the case that the best prior available for genome resequencing is one that we derive from first principles. 
Large scale genomic surveys have sequenced hundreds of thousands of humans, and where possible have deposited results into the public domain \cite{1000Gphase1, 1000g2015, exac2015, cavalli2005human, uk10k2015uk10k}.
The NIST Genome in a Bottle consortium \cite{zook2014integrating} has developed a reference standard for genome resequencing based around NA12878 (a cell line derived from a HapMap participant \cite{gibbs2003international}), which provides an accurate set of labeled data for the evaluation of genomic analyses.
Here, we develop a model capable of using these resources to learn a general model for genotyping.
%To utilize this rapidly-deepening catalog of variation in a new individuals, we built a supervised learning task to genotype by example.
% this has been attempted before, as in GATK VQSR or SNPsvm TODO: cites? do we beat VQSR?
%To attack this task we develop a novel set of feature representations, use the complementary strengths of two variant callers,  achieving state of the art performance.

% The system trained in this way, using the complementary strengths of two variant callers, freebayes and fermkit, combined with a novel set of feature representations inspired by genomic debugging tools and graphical reductions of genome variation

%President Obamaâ€™s Precision Medicine Initiative envisions a day when an individual's medical care will be tailored in part based on their unique characteristics and genetic make-up.
%The goal of the FDA's first precisionFDA challenge is to engage the genomics community in advancing the quality standards in order to achieve more consistent results in the context of genetic tests (related to whole human genome sequencing), advancing the goal of better personalized care.
%PrecisionFDA invites all innovators to take the challenge and assess their software on the supplied reference human datasets. Participation is voluntary, but instrumental in helping the community prepare for the coming genomic data revolution.

The PrecisionFDA challenge is a publicly-funded initiative to improve the quality and consistency of genetic tests through the engagement of the genomics community in a series of open challanges.
Its objective is to bring the same kind of rigorous public testing that has been used by the machine learning community to the medical genomics community.
We have applied our approach to the initial phase of the challenge (the ``Consistency'' challenge) and achieve the highest precision of any method which explicitly did not use the truth set in any phase of the submission process.\footnote{There was an entry that achieves 100 \% precision and recall by simply repeating the evlaution set, which was sadly distributed at the beginning of the challenge. Other submissions provide no more than vague reference to a ``ML'' approach used to filter, and do not exclude the use of the truth set for final decision boundary selection.}

%Our models base approach achieving the highest precision in the initial challenge (The precision challenge) of an entry that explciitly does not use the evaluation set in its training or paramter tunning 

%TODO https://precision.fda.gov/comparisons/1030 is problematic since they claim not to be using the test set (for training, i cosen my words crefully nd i assume so did they) 


We require a corpus of examples of sequencing data from genomic loci where we have a strong estimate of the actual genome state. %TODO: edit flow with upper part
% this is available in NIST genome in a bottle
% background of problem
% history: 1000G, data formats, NIST
% current approaches 
% reference guided: bwa / aligners -> samtools, freebayes, platypus, GATK % assembly based: cortex, sga, fermikit
 similar techniques: GATK's VQSR (variant quality score recalibration) and SNPSVM (mention), but not a challenge particpant)
% VSQR provide rough idea of what is different http://gatkforums.broadinstitute.org/gatk/discussion/39/variant-quality-score-recalibration-vqsr
The performance is comparable to that achieved by GATK with the recomended methodology; it offers several adavantges; libre, much smaller human effort, tunable to new sequencing tenchnologies (TODO: so is GATK's VQSR ...) 

The literature on genotyping is vast, and a full review of it is outside the scope of this paper.
Within the NIPS comunity \cite{teh2011modelling}, develop Bayesian nonparametric models for sequential data, the fragmentation-coagulation processes (FCPs), and a discrete time variant that mantains perofrmance while being more scalable. \cite{elliott2012scalable}
% its equilibrium distributions are given by the Chinese restaurant,  avoid label switching non-identifiability problems develop an efficient Gibbs sampler for FCP motivated by applications in population genetics, and we demonstrate the utility of FCPs on problems of genotype imputation with phased and unphased SNP data
%http://papers.nips.cc/paper/4782-scalable-imputation-of-genetic-data-with-a-discrete-fragmentation-coagulation-process
%We present a Bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a Markov model of partitions. The partitions at consecutive locations in the genome are related by their clusters first splitting and then merging. Our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [Teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable. We apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [Teh et al 2011].
In contrast, we take a discriminative instead of generative approach, and show that existing techniques as developed for large scale classification tasks can achieve state of the art performance when used with apropiately constructed features. 
As far as the methods have been described, none of the submissions to the FDA challenge so far appear to explicitly use these methods, nor come from groups that do so.

The discriminative approach has been used before in the inersection of machine learning and genomics, notably the Pascal learning challenge included a classficication task based on dna split point detection.
In contrast to that challenge that used a small set of 200 features, we explore models with up to hundreds of thousands of non-zero features per sample.
Further, the methods presented here are evauated not only in terms of performance metrics on the derived classification task, but rather as whole systems in open competition with custom designed software for the task that embodies hundreds of human years of labor TODO: cite gatk effort? jsutify this claim somehow.

%Teh et al. (2011); Elliott & Teh (2012) showed that the Dirichlet diffusion tree (Neal, 2003b) is the fragementation-coagulation dual (Pitman, 2006; Bertoin, 2006) of the Kingman coalescent (Kingman, 1982; Teh et al., 2007). The stochastic process introduced therein can be viewed as combining the dual processes in order to model a time-evolving partition of objects. One could investigate if such a dual process exists for the beta diffusion tree or some variant thereof, from which a model for time evolving feature allocations could be obtained.

TODO: how related is this? \cite{kirkpatrick2012bayesian} % http://papers.nips.cc/paper/4602-bayesian-pedigree-analysis-using-measure-factorization
more broadly the " gene expression microarrays, " task % https://scholar.google.cz/scholar?q=gene&btnG=&hl=en&as_sdt=2005&sciodt=0%2C5&cites=15503184557873643836&scipsc=1


%Application papers should describe your work on a "real" as opposed to "hypothetical" application; specifically, it should describe work that has direct relevance to, and addresses the full complexity of, solving a non-trivial problem. Authors are also encouraged to convey insight about the problem, algorithms, and/or application

% elucidate (through an ablative analysis/lesion analysis, which removes one component of an algorithm at a time) which were the key components of the system needed to get the application to work. 
The features provided by the two underlying aligners, combined with features that characterize the alignmnt itself, can be used to make a a good genotyper.



%A NIPS application paper should be comparable in quality to paper in the corresponding application domain conference: for example, a text paper should be acceptable to SIGIR, EMNLP, or other appropriate conference Application papers should not only present concrete application results, but also contain at least one of the below elements:

%Applications that couldn't previously be done, at all, or on this scale
TODO can we argue somethign of what we are doing could be done before? at least not with libre software? or possibly theres some way to argue this.


%Techniques shown to be uniquely fitted to specific popular applications, leading to improved performance or more accurate solutions
%our strong point

%Insights that, from the perspective of machine learning, distinct applications X and Y, whose respective users have never talked to each other, are the same.
%cant see anythign like this, but leaving it for inspiration




%dna/gene literature in NIPS



\section{From sequencing machine to feature representation}

We transform the short reads derived from an Illumina sequencing platform into a feature space representation suitable for use by generic machine learning techniques in two phases.
In the first phase, we use standard techniques for the processing of sequencing data to project sequencing information from a new sample onto the reference genome.
We then use alignment and assembly based genotyping programs to derive a list of putatively variable sites and non-reference alleles at these sites.
In the second phase, we convert the alignments, assembly, and candidate alleles derived from the first phase into a text-based feature representation compatible with the Vowpal Wabbit.
The two steps are decoupled, allowing us to change the inputs to the feature space transformation by utilizing other tools or sources for candidate alleles.
The process can thus be understood as a generic ensemble genetic variant detection and genotyping platform, however in this application we explore the use of only a small set of sources for this information.

\subsection{Alignment, assembly, and variant calling}

We align reads to a reference genome using {\tt bwa mem} \cite{li2013bwamem} and mark PCR duplicates with {\tt bamsormadup} \cite{tischler2014biobambam}.
We then use {\tt freebayes} \cite{garrison2012haplotype} to derive candidate alleles and genotypes for all sites where we observe more than 2 reads supporting a non-reference allele that comprising at least 5\% of the sequencing data at the locus.
In parallel, we generate an assembly of the raw sequencing reads using {\tt fermikit} \cite{li2015fermikit}.
We then map uniquely contiguous stretches of non-branching sequence in the overlap graph (unitigs) back to the reference genome, and use these alignments to identify candidate alleles with a genotyping approach implemented in the {\tt fermikit} toolchain.
In addition to a set of per-site annotations about the read evidence that they are presented, both {\tt fermikit} and {\tt freebayes} provide a quality estimate in their outputs which is approximately the posterior probability of variation at the locus under their Bayesian models.
We threshold the results at an extremely low posterior probability ($\approx 10^{-8}$) so as to certain false positives that only increase the computational burden of learning.\footnote{When comparing to the NIST Genome in a Bottle truth set we observed no true positives in this region of the data.}
Finally, use tools in {\tt vcflib} to merge the results into a single set of candidates \cite{vcflibgit}, retaining software level annotations provided by both systems.

This toolchain is the simplest that we are aware of that combines the two main forms of genotyping in common use.
{\tt freebayes} implements a haplotype-based genetic variant detection process that examines only reads locally mapped to a particular locus, and is limited to detecting genetic variants that are approximately the same size as a single read.
This provides in high sensitivity to small variation.
In contrast, {\tt fermikit} compresses sequencing data into a string graph \cite{myers2005}, then projects the unique portions of this graph onto the reference to find potential sequence variants.
The compression reduces the sensitivity of the method to shorter sequence variants, which resemble sequencing errors, but through assembly of long stretches of DNA it is able to detect very large sequence variants, such as insertions and deletions of 10s of thousands of base pairs.
Both methods and the integration process we use are fully implemented using free software with permissive licensing, unlike similar solutions such as the {\tt GATK} \cite{gatk2011}, which will enable their free use by other researchers.

\subsection{A generic feature representation for genotyping}

Existing learning techniques to approximate $f(S) \to G$ only use aggregate statistics describing $S$ at the level of a single genomic site.
For instance, SNPSVM applies a support vector machine to site level annotations such as read depth and mean sequencing base quality \cite{o2013support}.
The {\tt GATK} Variant Quality Score Recalibrator (GATK-VQSR) uses a very similar feature space, but develops a site level quality ``recalibration'' by applying a guassian mixture model built from distributions learned on many genomes to estimate outlier variants given a known set of true positives.
The full read information $S$ is apparently considered to expensive to directly use as an input to learning.

However, we observed that bioinformaticians would often evaluate the quality of genotyping results using visual representations of read alignments such as those implemneted in {\tt IGV} or {\tt samtools tview} \cite{robinson2011integrative, samtools}.
The human-driven assertion that the site was ``good'' or ``bad'' based on the appearance of the alignments suggested that we could learn to genotype directly from the alignments.
Following this inspiration, we designed a feature representation that retains all relevant features of $S$.
We implement this transformation in a software tool that consumes alignments (in BAM format), candidate variants (in VCF format), a window size parameter $w$, and a maximum depth $d$ to produce a feature representation for each candidate containing a description of: the reference genome; candidate alleles from the VCF file; alignments and their matches to the candidate alleles and reference; software provided annotations from the VCF file; and a novel compression of the sequencing data against a sequence graph implied by the variant record.

\subsubsection{The alignment matrix}

% hra

At each locus we generate a mutually gapped reference-based multiple sequence alignment (MSA) of the alignments, reference, and candidate alleles from the VCF file using the same algorithm as that implemented in {\tt samtools tview}.
We build the matrix using the alphabet $\{ {\tt A, T, G, C, N, U, M, R, S} \}$.
$\{ {\tt A, T, G, C} \}$ represent the standard DNA bases where the alignment suggests a candidate single base change (SNP), ${\tt N}$ is a degenerate character that could represent any of the standard bases, ${\tt U}$ is a gap character used to pad the alignment matrix where sequences have relative insertions or deletions (indels), ${\tt M }$ indicates missing data, ${\tt R}$ represents a reference-matching base in an alignment, and ${\tt S}$ indicates a ``soft clip'' or insertion at the beginning or end of an alignment.
This MSA is represented in matrix form, with its width equal to the window size and depth bounded by the number of alignments or an optional parameter that caps the number of alignments passed through the feature transformation.

To build the mutually gapped MSA, we first record all unique edits between the reference genome and the alignments.
Then, we determine the edit with the longest insertion length relative to the reference at each position and gap the alignments and reference at that position by adding the gap character ${\tt U}$ to them where their sequence length at the position is less than the maximum insertion length.
We use the same gap character to indicate relative deletions between the alignments and the reference.
We pad the alignments and candidate alleles with ${\tt M}$ where they do not fully overlap the window.

The mutual gapping step may change the width and centering of the matrix, so we must re-center and bound it.
We center the matrix such that the first base pair in the VCF record which we are working from is $w/2$ characters from the left hand edge of the window.
Finally, we re-bound the matrix so that it contains $w$ columns.
In effect, the variation is always represented in the right side of the matrix.

We build the matrix in the same way for small variants such as SNPs as we do for longer variants such as indels.
Although much of a longer indel will not be represented, we can use the first $w/2$ base pairs of it when genotyping.
For a typical window size $w = 16$, this would provide us 8 base pairs of information from each alignment.
As most indels are short, this will only harm resolution for long indels.

\subsubsection{Candidate-relative alignment sort}

% m

We sort the alignment matrix in a way that provides an approximate invariance relative to matches between the alignments and the candidate alleles.
For each alignment, we determine its pairwise identity with all candidate alleles.
For each candidate allele, we then sort the alignments in descending order by their match rate to the candidate alleles.
We then project the gapped alignments from the MSA into one matrix per allele where each matrix contains the top $d$ alignments sorted by match with that allele.
Alignments that do not match any candidate allele are sorted into another matrix, as are alignments which contain soft clips.
We sort soft clips separately because they are used by several methods to detect larger variants \cite{kronenberg2015wham}, which we hypothesized would help when genotyping variants detected by assembly with {\tt fermikit}.
All alignment-relative features are provided in this sort order.
We provide the match rates between each alignment and each candidate allele in the same order as the the alignments are sorted in the candidate-relative invariant sort.

\subsubsection{Software-provided annotations}

% s

We map software provided annotations provided in the VCF directly into a feature space in our representation.
Thes features include the estimated posterior probability of variation at the locus, the read depth, the sum of base qualities in the alignments, and other aggregate point statistics that the method developer has provided.

\subsubsection{Variation graph compression}

% k

The mutually gapped alignment matrix is susceptible to noise in the alignments.
Indel errors influence the representation of all other alignments, which makes the representation unstable.
Additionally, as we use the reference-based alignments to build up the alignment matrix representation, the representation is susceptible to errors in alignment.
As the alignments are derived between the reference and each read, they do not benefit from the information about candidate variation.
We resolve these issues by generating a ``variation graph'' that exactly represents the VCF record and aligning the reads against it in order to generate a compressed and fully normalized representation of the aggregate relationship between the alignments and candidate alleles.

We define a variation graph to be a graph with embedded paths $G = ( N, E, P )$ comprised of
a set of nodes $N = n_1 \ldots n_M$,
a set of directed edges $E = e_1 \ldots e_L$,
and a set of paths $P = p_1 \ldots p_Q$ which describe transformations from the graph space into a set of sequences.
Each node $n_i$ represents a sequence $seq(n_i)$ which is built from an arbitrary alphabet ${ \cal A } = \{ {\tt A, T, G, C, N} \}$.
Edges represent linkages between nodes that are allowed to be followed in representing longer sequences as paths through the graph.

We are interested in alignments to this graph, which we describe as alignments to a walk (or path) through the graph.
Explicitly, a path is a series of ``mapping'' operations, each describing the segment of the path derived from a single node, $p = m_1, \ldots, m_{|p|}$.
Each mapping $m$ can be written as $( (n, o), e_i \ldots e_{|m|} )$, where $n$ is the node, $o$ is the zero-offset start position on the node ($0 \le o < |n|$), and each $e_i$ is an ``edit'' which copies or replaces a segment of the node sequence.
This model is a straightforward generalization of the pairwise alignments generated by reference-based aligners to operate on a graph.

For each site we construct a graph $G$ such that the reference and alleles are represented as paths through the graph.
We construct the graph such that the alleles in the VCF record are whole nodes and the reference flanks around the locus are the single head and tail nodes of the graph.
We then collect reads that map within a larger window $g : g > w$ around the center of our window and realign them to $G$ using an efficient implementation of partial order alignment (POA) \cite{lee2002POA}.\footnote{In practice we use $g = 100$.}
As we aim to obtain a relativistic description of the strength of match between the alignments and the specific alleles at the locus, we ignore any edits in the alignments.
We then count the number of alignments mapping to each allele node, and normalize these counts by the total number of alignments mapping to any allele node.
This results in a weight $k$ for each allele node such that $k_i \in [0, 1] \forall n_i \in N$ and $\sum_{i = 0}^{|A|} k_i = 1$, which we provide in the feature representation.

\subsection{Application to PrecisionFDA Consistency Challenge}

% notes from precisionFDA
For the PrecisionFDA Consistency Challenge, participants were tasked with producing high-quality and reproducible genotyping results for a new sequencing data set derived from the canonical NA12878 reference cell line.
In effect, the objective was to test the ability of methods to generalize across different sequencing runs.\footnote{An ongoing challenge, the ``Truth'' challenge, seeks to measure generalization across individuals. It will close on May 26th, so we are unable to provide results here.}

To answer the challenge, we trained the models using three previous Garvan Institute sequencing runs on NA12878.\footnote{We used data available at https://www.garvan.org.au/research/kinghorn-centre-for-clinical-genomics/clinical-genomics/sequencing-services/sample-data for Robot\_1, Robot\_3, and Robot\_4.}
We labeled the candidate loci with their genotype status in the NIST Genome in a Bottle v2.19 truth set for NA12878 and use the learning procedure described in \ref{sec:learning} to train a model using only the previously-available data from the Garvan Institute.
We generated the feature transformation in the same manner for the challenge test data (Vial\_1), and derived labels for the candidates using the model trained on the previously-available data.
Finally, we convert the results into the VCF format and uploaded them into the PrecisionFDA testing framework.
There we used a remote comparison tool hosted by the organizers to evaluate the results.
This provides three metrics: precision (the percent of correct genotypes in the total set), recall (the fraction of precise genotype and allele assignments out of the total possible in the truth set) and an F-measure that combines the two under the assumption of equal weight for false positives and false negatives.
In our submission we achieve a precision of 99.54\%, recall of 98.91\% and an F-measure of 99.22\%.
Furthermore, we achieve the highest precision of any group that explicitly did not use the truth set (Vial\_1) in selecting parameters or thresholds for the submission.

\section{Learning}
\label{sec:learning}

The learning from the data is implemented without explicit regularization penalty. The capacity control is instead achieved by stopping the SGD when the holdout error rate is not reduced over three iterations \citep{hardt2015train}.
The main reason behind this choice was to avoid hyper-parameter tuning given the computational cost of fitting the richer models.


\begin{equation}
  \min_{\w \in \R^d}~~\sum_{i=1}^n\ell(\w^\tr\x_i;\,y_i) 
  \label{eqn:objective}
\end{equation}

%TODO CPU hours for the stage poly?
Where the feature vector $x$ incudes the different terms presented in the pipeline, 

Error-correcting tournaments \citep{beygelzimer2009error} where used , suprisingly given the small number, 7, of classes showed better performance in the holdout than the standard one against all reductions. 
Feature hashing and stochastic gradient descent using adaptive normalized and invariant update scheme as implemented in Vowpal Wabbit \cite{mcmahan2010adaptive, duchi2011adaptive, agarwal2014reliable} 



\section{Results}

Our best model achieved the highest precision submission among those that explicitly dont use the test set in their learning or tuning.\footnote{ Without this restriction there is a completely accurate fiel submitte,d as well as several methods whose peroformance is unbelievably good TODO: show more explcitily why t is implausible, and where the degree to which the adjustments are tuned to produce good results in the evaluation set is left unsaid.}

\subsection{Baselines \& Ablations }
 

\begin{table}[]
  \centering
  \caption{Ablation results}
  \label{ablation-results}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    name & 0-1 loss & feature spaces & interactions & options \\ \hline
    Software-provided annotations & 0.012204 & s & ss & \\
    Reference  & 0.407825 & r & rr &\\
    Alleles read & 0.016569 & a & & \\
    Haplotype & 0.416780 & h & hh & \\
    Variation graph compression & 0.035268 & k & kk & \\
    Candidate-relative alignment sort & 0.024088 & m & mm & \\
    Cand. alignment and software & 0.009975 & ms & mm  ss  ms & \\
    Variation graph and Cand. Align. & 0.021169 & km & kk  mm  km & \\
    ... and software & 0.009639 & kms & kk  km  mm  ms  ss & \\
    ... and soft. and reads & 0.008795 & kmsa & kk  km  mm  ms  ss & \\
    with haplotypes & 0.009060 & kmshr & kk  km  mm  ms  ss  hr & \\
    .... 2-grams  & 0.009205 & kmshr & kk  km  mm  ms  ss  hr & ngram 2 \\
    ... 3-grams & 0.009368 & kmshr & kk  km  mm  ms  ss  hr & ngram 3 \\
    ... allel and  & 0.008799 & kmshra & kk  km  mm  ms  ss  hr & ngram 2 \\
    4b31 & 0.012261 & kmshr & & stage-poly \\
    \hline
  \end{tabular}
\end{table}

% can't do ... maybe we report it -- effect of using "clever" featuranmes for aln relative to just some natural ordering.


\section{Generalization}

\subsection{Across sequencing runs}



\subsection{Accross individuals}
where there are actually reads we can use to train and test on

%zcat 1kg/HG00403.hhga.gz | head -1000000 | grep 'aln0.4\|aln1.4\|aln2.4' | vw --ect 7 --ignore sg --cache_file x.cache -k --passes 10 -q kk -q mm -f x.model

makes sure we have 4 alignments in at least one allele bin

\subsection{Accross regions}
there is a better way
we can hold out specific regions
but the individual
at this level
is DNA that was synthesized in a test tube

it is not like there is something special about it
we can show that we don't need to memorize sites
TODO:SHOW

that's enough to make a strong claim that you would generalize to a new sample with different variants in different places
if you want to go so far
obviously it has caveats but if you want to explore this that's the cleanest way

that DNA is a molecule and when a person has a variatn in a new position it is indistinguishable from another variant we held out from the current individual

the itner-individual comparison in the 1000G is fraught too
because the samples were all genotyped in a massive joint assembly process
so it's hard to claim they are independent

\section{Conclusion}

Using simple, shallow learners, and leveraging the work that has been applied to large scale classification tasks combined with handcrafted feature representations, we show state of the art performance in genotyping task.



\section{Further Work}
incoporating 1000G


%somehow bring in?
%Teh et al. (2011); Elliott & Teh (2012) showed that the Dirichlet diffusion tree (Neal, 2003b) is the fragementation-coagulation dual (Pitman, 2006; Bertoin, 2006) of the Kingman coalescent (Kingman, 1982; Teh et al., 2007). The stochastic process introduced therein can be viewed as combining the dual processes in order to model a time-evolving partition of objects. One could investigate if such a dual process exists for the beta diffusion tree or some variant thereof, from which a model for time evolving feature allocations could be obtained.


\small

\bibliographystyle{plain}
\bibliography{geno}

\end{document}


